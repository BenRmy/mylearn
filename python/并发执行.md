# 多线程、多进程、协程

## threading--基于线程的并行

threading模块是Python里面常用的线程模块，多线程处理任务对于提升效率非常重要。这个模块在较低级的模块 _thread 基础上建立较高级的线程接口。线程是计算机中执行任务的最小单元。一般IO密集型应用使用多线程（不使用CPU）。

+ 线程优点：共享内存，IO操作时相当于并发操作
+ 线程缺点：抢占资源时需要锁

线程不是越多越好，需要具体分析，因为线程切换也需要时间。threading模块可以创建多个线程，不过由于GIL锁的存在，Python在多线程里面其实是快速切换。所以一般用于IO密集型应用。

基本使用示例：

```python
import time
import threading


def ff(a1, a2):
    time.sleep(5)  # 代表IO耗时操作
    # 其他操作或函数动作


# 下面代码是直接运行下去的，不会等待函数里面设定的sleep
t = threading.Thread(target=ff, args=(111, 112))  # 创建线程
# 设置为后台线程，默认是False，设置为True之后则主线程不用等待子线程。注意与join()的区别
t.setDaemon(True)
t.start()  # 开启线程

t = threading.Thread(target=ff, args=(111, 112))
t.start()

t = threading.Thread(target=ff, args=(111, 112))
t.start()
```

在线程里面setDaemon()和join()方法都是常用的，他们的区别如下：

+ join()方法：主线程A中，创建了子线程B，并且在主线程A中调用了B.join()，那么，主线程A会在调用的地方等待，直到子线程B完成操作后，才可以接着往下执行，那么在调用这个线程时可以使用被调用线程的join方法。join([timeout]) 里面的参数时可选的，代表线程运行的最大时间，即如果超过这个时间，不管这个此线程有没有执行完毕都会被回收，然后主线程或函数都会接着执行的，如果线程执行时间小于参数表示的时间，则接着执行，不用一定要等待到参数表示的时间。
+ setDaemon()方法。主线程A中，创建了子线程B，并且在主线程A中调用了B.setDaemon(),这个的意思是，把主线程A设置为守护线程，这时候，要是主线程A执行结束了，就不管子线程B是否完成,一并和主线程A退出.这就是setDaemon方法的含义，这基本和join是相反的。此外，还有个要特别注意的：必须在start() 方法调用之前设置，如果不设置为守护线程，程序会被无限挂起，只有等待了所有线程结束它才结束。

线程锁：在多线程处理任务的时候，在同时操作一个数据的时候可能会造成脏数据，这时候就出现了锁的概念，也就是有一个线程在操作该数据的时候，就把该数据锁上，防止别的线程操作，操作完了再释放锁。一个锁是对于一个资源而言的，每个资源可对应一个锁，即可以存在多个资源锁。

使用示例：

```python
import threading
import time

n = 0

class MyThread(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)

    def run(self):
        global n, lock
        time.sleep(1)
        if lock.acquire():
            print(n, self.name)
            n += 1
            lock.release()

if "__main__" == __name__:
    n = 1
    ThreadList = []
    lock = threading.Lock()
    for i in range(1, 200):
        t = MyThread()
        ThreadList.append(t)
    for t in ThreadList:
        t.start()
    for t in ThreadList:
        t.join()
```

这种类型的加锁可能会导致死锁发生，Threading模块中，也有一个类，RLock，称之为可重入锁。该锁对象内部维护着一个Lock和一个counter对象。counter对象记录了acquire的次数，使得资源可以被多次require。最后，当所有RLock被release后，其他线程才能获取资源。在同一个线程中，RLock.acquire可以被多次调用，利用该特性，可以解决部分死锁问题。

event方法：该方法的具体用法是给线程设置红绿灯，红灯表示停，绿灯表示运行。可以用于等待输入。

使用示例：

```python
import threading
import time


def do(event):
    print('start')
    event.wait()  # 红灯，所有线程执行都这里都在等待
    print('end')

event_obj = threading.Event()  # 创建一个事件

for i in range(10):  # 创建10个线程
    t = threading.Thread(target=do, args=(event_obj,))
    t.start()

time.sleep(5)

event_obj.clear()  # 让灯变红，默认也是红的，阻塞所有线程运行
data = input('请输入要：')
if data == 'True':
    event_obj.set()  # 变绿灯，可以运行
```

## multiprocessing--基于进程的并行

由于GIL的存在，如果想要充分地使用多核CPU的资源，在python中大部分情况需要使用多进程（并发执行）。multiprocessing支持子进程、通信和共享数据、执行不同形式的同步，提供了Process、Queue、Pipe、Lock等组件。

一般计算密集型应用使用多进程（使用多个CPU）。与threading.Thread类似，它可以利用multiprocessing.Process对象来创建一个进程。该进程可以运行在Python程序内部编写的函数。该Process对象与Thread对象的用法相同，也有start(), run(), join()的方法。此外multiprocessing包中也有Lock/Event/Semaphore/Condition类 (这些对象可以像多线程那样，通过参数传递给各个进程)，用以同步进程，其用法与threading包中的同名类一致。所以，multiprocessing的很大一部份与threading使用同一套API，只不过换到了多进程的情境。

+ 进程优点：同时利用多个CPU，能够同时进行多个操作  
+ 进程缺点：耗费资源（每个进程有单独的内存空间）  

进程不是越多越好，一般：进程数=CPU数。在使用这些共享API的时候，要注意以下几点：

+ 在UNIX平台上，当某个进程终结之后，该进程需要被其父进程调用wait，否则进程成为僵尸进程(Zombie)。所以，有必要对每个Process对象调用join()方法 (实际上等同于wait)。对于多线程来说，由于只有一个进程，所以不存在此必要性。
+ multiprocessing提供了threading包中没有的IPC(比如Pipe和Queue)，效率上更高。应优先考虑Pipe和Queue，避免使用Lock/Event/Semaphore/Condition等同步方式 (因为它们占据的不是用户进程的资源)。
+ 多进程应该避免共享资源。在多线程中，可以比较容易地共享资源，比如使用全局变量或者传递参数。在多进程情况下，由于每个进程有自己独立的内存空间，以上方法并不合适。此时可以通过共享内存和Manager的方法来共享资源。但这样做提高了程序的复杂度，并因为同步的需要而降低了程序的效率。
+ window系统下，需要注意的是要想启动一个子进程，必须加上那句if __name__ == "main"，进程相关的要写在这句下面。
+ Process.PID中保存有PID，如果进程还没有start()，则PID为None。

Pool对象，它提供了一种方便的方法，可以跨多个输入值并行化函数的执行，跨进程分配输入数据（数据并行）。

```python
from multiprocessing import Pool

def f(x):
    return x*x

if __name__ == '__main__':
    with Pool(5) as p:
        print(p.map(f, [1, 2, 3]))
```

Process 类：在multiprocessing中，通过创建一个Process对象然后调用它的start()方法来生成进程。Process和threading.Thread的API相同。

```python
from multiprocessing import Process

def f(name):
    print('hello', name)

if __name__ == '__main__':
    p = Process(target=f, args=('bob',))
    p.start()
    p.join()
```

上下文和启动方法：根据不同的平台，multiprocessing 支持三种启动进程的方法。这些启动方法有：

+ spawn：父进程启动一个新的Python解释器进程。子进程只会继承那些运行进程对象的run()方法所需的资源。特别是父进程中非必须的文件描述符和句柄不会被继承。相对于使用fork或者forkserver，使用这个方法启动进程相当慢。可在Unix和Windows上使用。 Windows上的默认设置。
+ fork：父进程使用os.fork()来产生Python解释器分叉。子进程在开始时实际上与父进程相同。父进程的所有资源都由子进程继承。请注意，安全分配多线程进程是棘手的。只存在于Unix。Unix中的默认值。
+ forkserver：程序启动并选择forkserver启动方法时，将启动服务器进程。从那时起，每当需要一个新进程时，父进程就会连接到服务器并请求它分配一个新进程。服务器分配进程是单线程的，因此使用os.fork() 是安全的。没有不必要的资源被继承。可在Unix平台上使用，支持通过Unix管道传递文件描述符。

要选择一个启动方法，应该在主模块的if __name__ == '__main__'子句中调用set_start_method()。

```python
import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    mp.set_start_method('spawn')
    q = mp.Queue()
    p = mp.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```

在程序中 set_start_method() 不应该被多次调用。或者可以使用 get_context() 来获取上下文对象。上下文对象与多处理模块具有相同的API，并允许在同一程序中使用多个启动方法。

```python
import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    ctx = mp.get_context('spawn')
    q = ctx.Queue()
    p = ctx.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```

注意：与一个上下文相关的对象可能与不同上下文的进程不兼容。特别是，使用 fork 上下文创建的锁不能传递给使用 spawn或forkserver 启动方法启动的进程。

进程间通信，multiprocessing支持进程之间的两种通信通道：

+ 队列：Queue 类是一个近似 queue.Queue的克隆。队列是线程和进程安全的。

```python
from multiprocessing import Process, Queue

def f(q):
    q.put([42, None, 'hello'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())    # prints "[42, None, 'hello']"
    p.join()
```

+ 管道：Pipe()函数返回一个由管道连接的连接对象，默认情况下是双工（双向）。Pipe()返回的两个连接对象表示管道的两端，每个连接对象都有send()和recv()方法（相互之间的）。请注意，如果两个进程（或线程）同时尝试读取或写入管道的同一端，则管道中的数据可能会损坏。当然，同时使用管道的不同端的进程不存在损坏的风险。

```python
from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())   # prints "[42, None, 'hello']"
    p.join()
```

进程间的同步:multiprocessing 包含来自threading的所有同步基本体的等价物。

例如，可以使用锁来确保一次只有一个进程打印到标准输出（不要使用来自不同进程的锁）：

```python
from multiprocessing import Process, Lock

def f(l, i):
    l.acquire()
    try:
        print('hello world', i)
    finally:
        l.release()

if __name__ == '__main__':
    lock = Lock()

    for num in range(10):
        Process(target=f, args=(lock, num)).start()
```

在进程之间共享状态：在进行并发编程时，通常最好尽量避免使用共享状态。使用多个进程时尤其如此。但是如果真的需要使用一些共享数据，那么multiprocessing提供了两种方法：

+ 共享内存：可以使用 Value 或 Array 将数据存储在共享内存映射中。创建num和arr时使用的'd'和'i'参数是array模块使用的typecode类型：'d' 表示双精度浮点数，'i' 表示有符号整数。这些共享对象将是进程和线程安全的。

```python
from multiprocessing import Process, Value, Array

def f(n, a):
    n.value = 3.1415927
    for i in range(len(a)):
        a[i] = -a[i]

if __name__ == '__main__':
    num = Value('d', 0.0)
    arr = Array('i', range(10))

    p = Process(target=f, args=(num, arr))
    p.start()
    p.join()

    print(num.value)
    print(arr[:])
```

+ 服务器进程：由Manager()返回的管理器对象控制一个服务器进程，该进程保存Python对象并允许其他进程使用代理操作它们。Manager()返回的管理器支持类型：list、dict、Namespace、Lock、RLock、Semaphore、BoundedSemaphore、Condition、Event、Barrier、Queue、Value和Array。服务器进程管理器比使用共享内存对象更灵活，因为它们可以支持任意对象类型。此外，单个管理器可以通过网络由不同计算机上的进程共享。但是，它们比使用共享内存慢。

```python
from multiprocessing import Process, Manager

def f(d, l):
    d[1] = '1'
    d['2'] = 2
    d[0.25] = None
    l.reverse()

if __name__ == '__main__':
    with Manager() as manager:
        d = manager.dict()
        l = manager.list(range(10))

        p = Process(target=f, args=(d, l))
        p.start()
        p.join()

        print(d)
        print(l)
```

使用工作进程池：Pool类表示一个工作进程池。它具有允许以几种不同方式将任务分配到工作进程的方法。请注意，池的方法只能由创建它的进程使用。

multiprocessing包大部分复制了threading模块的API。下面是一些参考。

class multiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)：进程对象表示在单独进程中运行的活动。Process类等价于 threading.Thread。应始终使用关键字参数调用构造函数。group应该始终是None，它仅用于兼容threading.Thread。target是由run()方法调用的可调用对象，它默认为None，意味着什么都没有被调用。name是进程名称。args是目标调用的参数元组。kwargs是目标调用的关键字参数字典，如果提供，则键参数daemon将进程daemon标志设置为True或False，如果是None（默认值），则该标志将从创建的进程继承。默认情况下，不会将任何参数传递给target。如果子类重写构造函数，它必须确保它在对进程执行任何其他操作之前调用基类构造函数Process.__init__()。以下为方法：

+ run()：表示进程活动的方法。可以在子类中重载此方法。标准 run() 方法调用传递给对象构造函数的可调用对象作为目标参数（如果有），分别从 args 和 kwargs 参数中获取顺序和关键字参数。
+ start()：启动进程。每个进程对象最多只能调用一次。它安排对象的 run() 方法在一个单独的进程中调用。
+ join([timeout])：如果可选参数 timeout 是None（默认值），则该方法将阻塞，直到调用join()方法的进程终止。如果 timeout 是一个正数，它最多会阻塞timeout秒。请注意，如果进程终止或方法超时，则该方法返回None。检查进程的exitcode以确定它是否终止。一个进程可以合并多次。进程无法并入自身，因为这会导致死锁。尝试在启动进程之前合并进程是错误的。
+ name：进程的名称。该名称是一个字符串，仅用于识别目的。它没有语义。可以为多个进程指定相同的名称。初始名称由构造器设定。如果没有为构造器提供显式名称，则会构造一个形式为 'Process-N1:N2:...:Nk' 的名称，其中每个 Nk 是其父亲的第 N 个孩子。
+ is_alive()：返回进程是否还活着。粗略地说，从 start() 方法返回到子进程终止之前，进程对象仍处于活动状态。
+ daemon：进程的守护标志，一个布尔值。这必须在start()被调用之前设置。初始值继承自创建进程。当进程退出时，它会尝试终止其所有守护进程子进程。请注意，不允许守护进程创建子进程。否则，守护进程会在子进程退出时终止其子进程。另外，这些不是Unix守护进程或服务，它们是正常进程，如果非守护进程已经退出，它们将被终止（并且不被合并）。

除了threading.Thread API，Process对象还支持以下属性和方法：

+ pid：返回进程ID。在生成该进程之前，这将是None
+ exitcode：子进程退出代码。如果进程尚未终止，这将是None。负值 -N 表示孩子被信号 N 终止
+ authkey：进程的身份验证密钥（字节字符串）。当 multiprocessing 初始化时，主进程使用 os.urandom() 分配一个随机字符串。当创建 Process 对象时，它将继承其父进程的身份验证密钥，尽管可以通过将 authkey 设置为另一个字节字符串来更改。
+ sentinel：系统对象的数字句柄，当进程结束时将变为 "ready" 。如果要使用 multiprocessing.connection.wait() 一次等待多个事件，可以使用此值。否则调用join()更简单。在Windows上，这是一个操作系统句柄，可以与 WaitForSingleObject和 WaitForMultipleObjects系列API调用一起使用。在Unix上，这是一个文件描述符，可以使用来自select模块的原语。
+ terminate()：终止进程。在Unix上，这是使用SIGTERM信号完成的；在Windows上使用 TerminateProcess()。请注意，不会执行退出处理程序和finally子句等。请注意，进程的后代进程将不会被终止--它们将简单地变成孤立的。警告：如果在关联进程使用管道或队列时使用此方法，则管道或队列可能会损坏，并可能无法被其他进程使用。类似地，如果进程已获得锁或信号量等，则终止它可能导致其他进程死锁。
+ kill()：与terminate()相同，但在Unix上使用SIGKILL信号。
+ close()：关闭 Process 对象，释放与之关联的所有资源。如果底层进程仍在运行，则会引发ValueError。一旦 close() 成功返回，Process对象的大多数其他方法和属性将引发 ValueError。

注意：start()、join()、is_alive()、terminate()和exitcode方法只能由创建进程对象的进程调用。

管道和队列：使用多进程时，一般使用消息机制实现进程间通信，尽可能避免使用同步原语，例如锁。消息机制包含：Pipe() (可以用于在两个进程间传递消息)，以及队列(能够在多个生产者和消费者之间通信)。当一个对象被放入一个队列中时，这个对象首先会被一个后台线程用pickle序列化，并将序列化后的数据通过一个底层管道的管道传递到队列中。这种做法会有点让人惊讶，但一般不会出现什么问题。如果它们确实妨碍了你，你可以使用一个由管理器manager创建的队列替换它。

+ multiprocessing.Pipe([duplex])：返回一对 Connection对象(conn1, conn2)，分别表示管道的两端。如果 duplex 被置为 True (默认值)，那么该管道是双向的。如果 duplex 被置为False，那么该管道是单向的，即conn1只能用于接收消息，而conn2仅能用于发送消息。
+ class multiprocessing.Queue([maxsize])：返回一个使用一个管道和少量锁和信号量实现的共享队列实例。当一个进程将一个对象放进队列中时，一个写入线程会启动并将对象从缓冲区写入管道中。一旦超时，将抛出标准库 queue 模块中常见的异常 queue.Empty 和 queue.Full。除了 task_done() 和 join() 之外，Queue 实现了标准库类queue.Queue中所有的方法。
+ class multiprocessing.SimpleQueue：这是一个简化的 Queue 类的实现，很像带锁的 Pipe。
+ class multiprocessing.JoinableQueue([maxsize])：JoinableQueue 类是 Queue 的子类，额外添加了 task_done() 和 join() 方法。

其他杂项：

+ multiprocessing.active_children()：返回当前进程存活的子进程的列表。调用该方法有“等待”已经结束的进程的副作用。
+ multiprocessing.cpu_count()：返回系统的CPU数量。该数量不同于当前进程可以使用的CPU数量。可用的CPU数量可以由len(os.sched_getaffinity(0))方法获得。可能引发 NotImplementedError。另外也可用os.cpu_count()。
+ multiprocessing.current_process()：返回与当前进程相对应的Process对象。
+ multiprocessing.freeze_support()：为使用了multiprocessing的程序，提供冻结以产生Windows可执行文件的支持。(在py2exe, PyInstaller 和 cx_Freeze 上测试通过)。需要在 main 模块的 if __name__ == '__main__' 该行之后马上调用该函数。
+ multiprocessing.get_all_start_methods()：返回支持的启动方法的列表，该列表的首项即为默认选项。可能的启动方法有'fork', 'spawn' 和forkserver。在Windows中，只有  spawn是可用的。Unix平台总是支持fork和spawn，且fork是默认值。
+ multiprocessing.get_context(method=None)：返回一个 Context 对象。该对象具有和 multiprocessing 模块相同的API。
+ multiprocessing.get_start_method(allow_none=False)：返回启动进程时使用的启动方法名。
+ multiprocessing.set_executable()：设置在启动子进程时使用的 Python 解释器路径。
+ multiprocessing.set_start_method(method)：设置启动子进程的方法。

## concurrent.futures--启动并行任务

该模块提供异步执行回调高层接口。异步执行可以由 ThreadPoolExecutor 使用线程或由 ProcessPoolExecutor 使用单独的进程来实现。两者都是实现抽像类 Executor 定义的接口。

class concurrent.futures.Executor：抽象类提供异步执行调用方法。要通过它的子类调用，而不是直接调用。

+ submit(fn, *args, **kwargs)：调度可调用对象fn，以fn(*args **kwargs)方式执行并返回 Future对象，代表可调用对象的执行。

```python
with ThreadPoolExecutor(max_workers=1) as executor:
    future = executor.submit(pow, 323, 1235)
    print(future.result())
```

+ map(func, *iterables, timeout=None, chunksize=1)：类似于 map(func, *iterables)，但是也有不同：立即收集iterables而不要延迟再收集；func是异步执行的且对func的调用可以并发执行。如果 func 调用引发一个异常，当从迭代器中取回它的值时这个异常将被引发。使用 ProcessPoolExecutor 时，这个方法会将 iterables 分割任务块并作为独立的任务并提交到执行池中。这些块的大概数量可以由 chunksize 指定正整数设置。 对很长的迭代器来说，使用大的 chunksize 值比默认值 1 能显著地提高性能。 chunksize 对 ThreadPoolExecutor 没有效果。
+ shutdown(wait=True)：当待执行的future完成执行后向执行者发送信号，它就会释放正在使用的任何资源。在关闭后调用Executor.submit()和Executor.map()会触发RuntimeError。如果 wait 为 True，则此方法只有在所有待执行的future完成执行且释放已分配的资源后才会返回。如果 wait为 False，方法立即返回，所有待执行的期程完成执行后会释放已分配的资源。不管 wait的值是什么，整个 Python程序将等到所有待执行的期程完成执行后才退出。

class concurrent.futures.ThreadPoolExecutor(max_workers=None, thread_name_prefix='', initializer=None, initargs=())：Executor的子类，使用最多 max_workers 个线程的线程池来异步执行调用。但是当回调已关联了一个 Future 然后再等待另一个 Future 的结果时就会发生死锁情况。initializer是在每个工作者线程开始处调用的一个可选可调用对象。如果max_workers为None或没有指定，将默认为机器处理器的个数，假如 ThreadPoolExecutor 侧重于I/O操作而不是CPU运算，那么可以乘以5，同时工作线程的数量可以比ProcessPoolExecutor的数量高。thread_name_prefix参数允许用户控制由线程池创建的threading.Thread工作线程名称以方便调试。

```python
import concurrent.futures
import urllib.request

URLS = ['http://www.foxnews.com/',
        'http://www.cnn.com/',
        'http://europe.wsj.com/',
        'http://www.bbc.co.uk/',
        'http://some-made-up-domain.com/']

# 请求单个页面并报告URL和内容
def load_url(url, timeout):
    with urllib.request.urlopen(url, timeout=timeout) as conn:
        return conn.read()

# 可以使用with语句来确保及时清理线程
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    # 启动加载操作并使用其URL标记每个future
    future_to_url = {executor.submit(load_url, url, 60): url for url in URLS}
    for future in concurrent.futures.as_completed(future_to_url):
        url = future_to_url[future]
        try:
            data = future.result()
        except Exception as exc:
            print('%r 产生了一个例外: %s' % (url, exc))
        else:
            print('%r page is %d bytes' % (url, len(data)))
```

class concurrent.futures.ProcessPoolExecutor(max_workers=None, mp_context=None, initializer=None, initargs=())：Executor 的子类，它使用一个最多可有 max_workers 个进程的进程池异步执行调用。ProcessPoolExecutor 使用 multiprocessing 回避 Global Interpreter Lock但也意味着只可以处理和返回可序列化的对象。__main__模块必须可以被worker子进程导入，这意味着 ProcessPoolExecutor 不可以工作在交互式解释器中。从提交给 ProcessPoolExecutor 的回调中调用Executor或Future方法会导致死锁。如果 max_workers 为None或没有指定，默认为机器的处理器个数。如果max_workers小于或等于0将会引发ValueError。mp_context 可以是一个多进程环境或None。它将用来启动工作者进程。如果 mp_context为None或没有指定，将使用默认多进程环境。nitializer 是在每个工作者进程开始处调用的一个可选可调用对象。如果其中一个工作进程被突然终止，BrokenProcessPool就会马上触发。可预计的行为没有定义，但执行器上的操作或它的期程会被冻结或死锁。mp_context 参数允许用户控制由进程池创建给工作者进程的开始方法。

```python
import concurrent.futures
import math

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]

def is_prime(n):
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

def main():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

if __name__ == '__main__':
    main()
```

Future对象

class concurrent.futures.Future：将可调用对象封装为异步执行。Future实例由Executor.submit() 创建，除非测试，不应直接创建。

+ cancel()：尝试取消调用。如果调用正在执行而且不能被取消那么方法返回False，否则调用会被取消同时方法返回True。
+ cancelled()：如果调用成功取消返回 True。
+ running()：如果调用正在执行而且不能被取消那么返回True。
+ done()：如果调用已被取消或正常结束那么返回 True。
+ result(timeout=None)：返回调用返回的值。如果调用还没完成那么这个方法将等待timeout秒。如果在 timeout 秒内没有执行完成，concurrent.futures.TimeoutError将会被触发。timeout可以是整数或浮点数。如果 timeout 没有指定或为None，那么等待时间就没有限制。如果 futrue 在完成前被取消则 CancelledError 将被触发。如果调用引发了一个异常，这个方法也会引发同样的异常。
+ exception(timeout=None)：返回由调用引发的异常。如果调用还没完成那么这个方法将等待 timeout 秒。如果在 timeout 秒内没有执行完成concurrent.futures.TimeoutError 将会被触发。其他和result()类似。
+ add_done_callback(fn)：附加可调用 fn 到futrue期程。当期程被取消或完成运行时，将会调用fn，而这个期程将作为它唯一的参数。加入的可调用对象总被属于添加它们的进程中的线程按加入的顺序调用。如果可调用对象引发一个Exception子类，它会被记录下来并被忽略掉。如果可调用对象引发一个BaseException子类，这个行为没有定义。如果期程已经完成或已取消，fn会被立即调用。

下面这些 Future 方法用于单元测试和 Executor 实现：

+ set_running_or_notify_cancel()：这个方法只可以在执行关联Future工作之前由Executor实现调用或由单元测试调用。
+ set_result(result)：将Future关联工作的结果给result。
+ set_exception(exception)：Future关联工作的结果给Exception exception。

模块函数：

+ concurrent.futures.wait(fs, timeout=None, return_when=ALL_COMPLETED)：等待 fs 指定的 Future 实例(可能由不同的 Executor 实例创建)完成。返回一个已被命名的二元元组集合。第一个集合被命名为done，包含等待完成前已完成的期程(正常结束或被取消)。第二个集合被命名为not_done，包含未完成的期程。return_when 指定此函数应在何时返回。它必须为以下常数之一：

常数|描述
--|--
FIRST_COMPLETED|函数将在任意可等待对象结束或取消时返回。
FIRST_EXCEPTION|函数将在任意可等待对象因引发异常而结束时返回。当没有引发任何异常时它就相当于 ALL_COMPLETED。
ALL_COMPLETED|函数将在所有可等待对象结束或取消时返回。

+ concurrent.futures.as_completed(fs, timeout=None)：通过fs指定的Future实例（可能由不同的 Executor 实例创建）返回一个迭代器，当它们完成时（正常结束或被取消）产生期程。任何被 fs 指定的重复期程都将会返回一次。首先产生调用 as_completed() 前已完成的期程。当__next__()调用以及从原始调用到as_completed()的时间超过timeout秒后结果还不可用时返回的迭代器就会引发 concurrent.futures.TimeoutError。timeout可以为 int 或 float 类型。如果 timeout 未指定或为None，则不限制等待时间。

## sched--事件调度器

class sched.scheduler(timefunc=time.monotonic, delayfunc=time.sleep)：一个实现通用事件调度程序的类。它需要两个函数来实际处理“外部世界”--timefunc应该可以在没有参数的情况下调用，并返回一个数字（“时间”，以任何单位表示）。如果time.monotonic不可用，则timefunc默认为time.time。delayfunc函数应该用一个参数可调用，与timefunc的输出兼容，并且应该延迟那么多时间单位。每个事件运行后，还会使用参数 0 调用 delayfunc，以允许其他线程有机会在多线程应用程序中运行。可以安全的在多线程环境中使用。

```python
import sched
import time

s = sched.scheduler(time.time, time.sleep)


def print_time(a='default'):
    print("From print_time", time.time(), a)


def print_some_times():
    print(time.time())
    s.enter(10, 1, print_time)
    s.enter(5, 2, print_time, argument=('positional',))
    s.enter(5, 1, print_time, kwargs={'a': 'keyword'})
    s.run()
    print(time.time())

print_some_times()
```

scheduler实例拥有以下方法和属性：

+ scheduler.enterabs(time, priority, action, argument=(), kwargs={})：安排一个新事件。time 参数应该有一个数字类型兼容的返回值，与传递给构造函数的 timefunc 函数的返回值兼容。计划在相同time的事件将按其priority的顺序执行 数字越小表示优先级越高。返回值是一个事件，可用于以后取消事件。
+ scheduler.enter(delay, priority, action, argument=(), kwargs={})：安排延后 delay时间单位的事件。除了相对时间，其他参数、效果和返回值与 enterabs() 的相同。
+ scheduler.cancel(event)：从队列中删除事件。如果event不是当前队列中的事件，则此方法将引发ValueError。
+ scheduler.empty()：如果事件队列为空，则返回真值。
+ scheduler.run(blocking=True)：运行所有预定事件。此方法将等待（使用传递给构造函数的delayfunc()函数）进行下一个事件，然后执行它，依此类推，直到没有更多的计划事件。如果 blocking 为false，则执行由于最快到期（如果有）的预定事件，然后在调度程序中返回下一个预定调用的截止时间（如果有）。action或delayfunc都可以引发异常。在任何一种情况下，调度程序都将保持一致状态并传播异常。如果action引发异常，则在将来调用 run() 时不会尝试该事件。如果一系列事件的运行时间比下一个事件之前的可用时间长，那么调度程序将完全落后。不会发生任何事件；调用代码负责取消不再相关的事件。
+ scheduler.queue：只读属性按照将要运行的顺序返回即将发生的事件列表。每个事件都显示为named tuple，包含以下字段：time、priority、action、argument、kwargs。

## queue--一个同步的队列类

该模块实现多生产者，多消费者队列。当信息必须安全的在多线程之间交换时，它在线程编程中是特别有用的。此模块中的 Queue 类实现了所有锁定需求的语义。其实现了三种类型的队列，它们的区别仅仅是条目取回的顺序。在FIFO队列中，先添加的任务先取回。在LIFO队列中，最近被添加的条目先取回(操作类似一个堆栈)。优先级队列中，条目将保持排序(使用heapq模块 )并且最小值的条目第一个返回。在内部，这三个类型的队列使用锁来临时阻塞竞争线程；然而，它们并未被设计用于线程的重入性处理。此外，模块实现了一个"简单的"FIFO队列类型 SimpleQueue，这个特殊实现为小功能在交换中提供额外的保障。

另：

+ multiprocessing.Queue：用于多进程上下文的队列类（而不是多线程）
+ collections.deque：无界队列的替代实现，具有快速原子的append()和popleft()操作，不需要锁定

模块包含的类和异常：

+ class queue.Queue(maxsize=0)：构造一个FIFO队列，maxsize可以限制队列的大小。如果队列的大小达到了队列的上限，就会加锁，加入就会阻塞，直到队列的内容被消费掉。maxsize的值小于等于0，那么队列的尺寸就是无限制的
+ class queue.LifoQueue(maxsize = 0)：构造一个Lifo队列
+ class PriorityQueue(maxsize = 0)：优先级最低的先出去，优先级最低的一般使用sorted(list(entries))[0])。典型加入的元素是一个元祖(优先级, 数据)
+ queue.empty异常：只有非阻塞的时候，队列为空，取数据才会报异常
+ queue.Full异常：只有非阻塞的时候，队列满了，继续放数据才会出现异常

Queue对象的公共方法：

+ Queue.qsize()：返回queue的近似值。注意：qsize>0 不保证(get)取元素不阻塞。qsize< maxsize不保证(put)存元素不会阻塞
+ Queue.empty():判断队列是否为空。和上面一样注意
+ Queue.full():判断是否满了。和上面一样注意
+ Queue.put(item, block=True, timeout=None): 往队列里放数据。如果满了的话，blocking = False 直接报Full异常。如果blocking = True，就是等一会，timeout必须为0或正数。None为一直等下去，0为不等，正数n为等待n秒还不能存入，报Full异常
+ Queue.put_nowait(item)：往队列里存放元素，不等待
+ Queue.get(item, block=True, timeout=None): 从队列里取数据。如果为空的话，blocking = False 直接报empty异常。如果blocking = True，就是等一会，timeout必须为0或正数。None为一直等下去，0为不等，正数n为等待n秒还不能读取，报empty异常
+ Queue.get_nowait(item)：从队列里取元素，不等待
+ Queue.queue.clear()：清空队列

下面两个方法跟踪排队的任务是否被守护的消费者线程完全的处理：

+ Queue.task_done()：表示前面排队的任务已经被完成。被队列的消费者线程使用。每个 get() 被用于获取一个任务，后续调用 task_done() 告诉队列，该任务的处理已经完成。如果join()当前正在阻塞，在所有条目都被处理后，将解除阻塞(意味着每个put()进队列的条目的task_done()都被收到)。如果被调用的次数多于放入队列中的项目数量，将引发ValueError异常 。
+ Queue.join()：阻塞至队列中所有的元素都被接收和处理完毕。当条目添加到队列的时候，未完成任务的计数就会增加。每当消费者线程调用task_done()表示这个条目已经被回收，该条目所有工作已经完成，未完成计数就会减少。当未完成计数降到零的时候，join() 阻塞被解除。

```python
def worker():
    while True:
        item = q.get()
        if item is None:
            break
        do_work(item)
        q.task_done()

q = queue.Queue()
threads = []
for i in range(num_worker_threads):
    t = threading.Thread(target=worker)
    t.start()
    threads.append(t)

for item in source():
    q.put(item)

# 阻止所有任务完成
q.join()

# stop workers
for i in range(num_worker_threads):
    q.put(None)
for t in threads:
    t.join()
```

SimpleQueue 对象提供下列描述的公共方法：

+ SimpleQueue.qsize()：返回队列的大致大小。注意，qsize() > 0 不保证后续的 get() 不被阻塞。
+ SimpleQueue.empty()：如果队列为空，返回 True，否则返回 False。如果 empty() 返回 False，不保证后续调用的 get() 不被阻塞。
+ SimpleQueue.put(item, block=True, timeout=None)：将item放入队列。此方法永不阻塞，始终成功（除了潜在的低级错误，例如内存分配失败）。可选参数block和timeout仅仅是为了保持 Queue.put() 的兼容性而提供，其值被忽略。
+ SimpleQueue.put_nowait(item)：相当于put(item)，仅为保持 Queue.put_nowait() 兼容性而提供。
+ SimpleQueue.get(block=True, timeout=None)：从队列中移除并返回一个项目。如果可选参数block为tru 并且timeout是None(默认值)，则在必要时阻塞至项目可得到。如果timeout是个正数，将最多阻塞timeout秒，如果在这段时间内项目不能得到，将引发Empty异常。反之(block是false), 如果一个项目立即可得到，则返回一个项目，否则引发 Empty 异常 (这种情况下，timeout 将被忽略)。
+ SimpleQueue.get_nowait()：相当于get(False)
